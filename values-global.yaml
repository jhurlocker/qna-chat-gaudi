---
global:
  pattern: opea-chatqna-on-gaudi
  options:
    useCSV: false
    syncPolicy: Automatic
    installPlanApproval: Automatic

  gaudillm:
    namespace: gaudi-llm
    build_envs: []
    runtime_envs: []
    
    tei_embedding:
      image: ghcr.io/huggingface/tei-gaudi:latest
      model_id: BAAI/bge-large-en-v1.5 # change to set custom TEI embedding model
#   uncomment to customize PVC size if using TEI models bigger thanBAAI/bge-large-en-v1.5
#      pvc:i
#        size: 10Gi
    tei_xeon:
      image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.2
      model_id: BAAI/bge-reranker-large # change to set custom TEI reranker model on CPU
#      pvc:
#        size: 10Gi
#    TODO: IMPLEMENT CUSTOMIZING TGI ENDPOINT FROM RHOAI

    servingRuntime:
      namespace: gaudi-llm
      name: tgi-70b-gaudi

    chatqna_gaudi_backend: 
      image: ghcr.io/huggingface/chatqna:latest
    chatqna_ui_server:  
      image: ghcr.io/huggingface/chatqna-ui:latest
    dataprep:  
      image: ghcr.io/huggingface/dataprep-redis:latest
    embedding:  
      image: ghcr.io/huggingface/embedding-tei:latest
    gaudillm_init: {}
    llm_server_for_gaudi:  
      image: ghcr.io/huggingface/llm-tgi:latest
    redis_vector_db:  
      image: ghcr.io/huggingface/redis/redis-stack:7.2.0-v9
    reranking:  
      image: ghcr.io/huggingface/reranking-tei:latest
    retriever:
      image: ghcr.io/huggingface/retriever-redis:latest
  
main:
  clusterGroupName: hub
  multiSourceConfig:
    enabled: true

